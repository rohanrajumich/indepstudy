{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from metrics import rouge_n, meteor, ter, chrf, wer, levenshtein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_predictions(predictions_path):\n",
    "    with open(predictions_path, 'r', encoding='utf-8') as pred_file:\n",
    "        predictions = [line.strip() for line in pred_file]\n",
    "    return predictions\n",
    "\n",
    "def load_references_from_test_split(dataset_name=\"rahular/itihasa\", split=\"test\"):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    references = [example[\"translation\"][\"sn\"] for example in dataset]\n",
    "    return references\n",
    "\n",
    "def calculate_metrics(prediction, reference):\n",
    "    metrics = {}\n",
    "    metrics['rouge1'] = rouge_n(prediction, [reference], n=1)['f1']\n",
    "    metrics['rouge2'] = rouge_n(prediction, [reference], n=2)['f1']\n",
    "    metrics['meteor'] = meteor(prediction, [reference])\n",
    "    metrics['ter'] = ter(prediction, [reference])\n",
    "    metrics['chrf'] = chrf(prediction, [reference])\n",
    "    metrics['wer'] = wer(prediction, reference)\n",
    "    metrics['levenshtein'] = levenshtein_distance(prediction, reference) / max(len(prediction), len(reference)) if max(len(prediction), len(reference)) > 0 else 0\n",
    "    return metrics\n",
    "\n",
    "def load_references_from_test_split(dataset_name=\"rahular/itihasa\", split=\"test\"):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    references = [example[\"translation\"][\"sn\"] for example in dataset if example[\"translation\"][\"sn\"].strip()]\n",
    "    return references\n",
    "\n",
    "def aggregate_metrics(predictions, references):\n",
    "    aggregated = defaultdict(list)\n",
    "    \n",
    "    for idx, (pred, ref) in enumerate(zip(predictions, references), start=1):\n",
    "        if not ref.strip():  # Skip if reference is empty\n",
    "            print(f\"Skipping empty reference at index {idx}\")\n",
    "            continue\n",
    "            \n",
    "        metrics = calculate_metrics(pred, ref)\n",
    "        for key, value in metrics.items():\n",
    "            aggregated[key].append(value)\n",
    "\n",
    "    average_metrics = {key: np.mean(values) * 100 for key, values in aggregated.items()}\n",
    "    return average_metrics, aggregated\n",
    "\n",
    "def plot_average_metrics(average_metrics, output_dir):\n",
    "    keys = list(average_metrics.keys())\n",
    "    values = list(average_metrics.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(keys, values, color='skyblue')\n",
    "    plt.xlabel('Score (%)')\n",
    "    plt.title('Average Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'average_metrics.png'))\n",
    "    plt.close()\n",
    "    print(\"Saved bar plot of average metrics.\")\n",
    "\n",
    "def plot_metric_distributions(aggregated_metrics, output_dir):\n",
    "    for metric, values in aggregated_metrics.items():\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.hist(values, bins=20, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{metric.capitalize()} Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_distribution.png'))\n",
    "        plt.close()\n",
    "        print(f\"Saved histogram for {metric}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions and references...\n",
      "Loaded 11722 predictions and 11721 references.\n",
      "Calculating metrics...\n",
      "\n",
      "Average Metrics:\n",
      "Rouge1: 8.69%\n",
      "Rouge2: 1.01%\n",
      "Meteor: 4.20%\n",
      "Ter: 118.06%\n",
      "Chrf: 25.64%\n",
      "Wer: 118.06%\n",
      "Levenshtein: 73.01%\n",
      "Saved average metrics to file.\n",
      "Saved bar plot of average metrics.\n",
      "Saved histogram for rouge1.\n",
      "Saved histogram for rouge2.\n",
      "Saved histogram for meteor.\n",
      "Saved histogram for ter.\n",
      "Saved histogram for chrf.\n",
      "Saved histogram for wer.\n",
      "Saved histogram for levenshtein.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/shared/3/projects/national-culture/cache/independent/cache/input-to-parse/checkpoint-1875\"\n",
    "predictions_path = os.path.join(model_dir, 'i2p_all_predictions.txt')\n",
    "output_dir = model_dir\n",
    "\n",
    "print(\"Loading predictions and references...\")\n",
    "predictions = load_predictions(predictions_path)\n",
    "references = load_references_from_test_split()\n",
    "print(f\"Loaded {len(predictions)} predictions and {len(references)} references.\")\n",
    "\n",
    "print(\"Calculating metrics...\")\n",
    "average_metrics, aggregated_metrics = aggregate_metrics(predictions, references)\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for metric, avg_score in average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {avg_score:.2f}%\")\n",
    "\n",
    "with open(os.path.join(output_dir, 'average_metrics.txt'), 'w') as f:\n",
    "    for metric, avg_score in average_metrics.items():\n",
    "        f.write(f\"{metric.capitalize()}: {avg_score:.2f}%\\n\")\n",
    "print(\"Saved average metrics to file.\")\n",
    "\n",
    "plot_average_metrics(average_metrics, output_dir)\n",
    "\n",
    "plot_metric_distributions(aggregated_metrics, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
