{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from metrics import rouge_n, meteor, ter, chrf, wer, levenshtein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_texts(predictions_path, references_path):\n",
    "    with open(predictions_path, 'r', encoding='utf-8') as pred_file:\n",
    "        predictions = [line.strip() for line in pred_file]\n",
    "    with open(references_path, 'r', encoding='utf-8') as ref_file:\n",
    "        references = [line.strip() for line in ref_file]\n",
    "    return predictions, references\n",
    "\n",
    "def calculate_metrics(prediction, reference):\n",
    "    metrics = {}\n",
    "    metrics['rouge1'] = rouge_n(prediction, [reference], n=1)['f1']\n",
    "    metrics['rouge2'] = rouge_n(prediction, [reference], n=2)['f1']\n",
    "    metrics['meteor'] = meteor(prediction, [reference])\n",
    "    metrics['ter'] = ter(prediction, [reference])\n",
    "    metrics['chrf'] = chrf(prediction, [reference])\n",
    "    metrics['wer'] = wer(prediction, reference)\n",
    "    metrics['levenshtein'] = levenshtein_distance(prediction, reference) / max(len(prediction), len(reference)) if max(len(prediction), len(reference)) > 0 else 0\n",
    "    return metrics\n",
    "\n",
    "def aggregate_metrics(predictions, references):\n",
    "    aggregated = defaultdict(list)\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        metrics = calculate_metrics(pred, ref)\n",
    "        for key, value in metrics.items():\n",
    "            aggregated[key].append(value)\n",
    "    average_metrics = {key: np.mean(values) * 100 for key, values in aggregated.items()}\n",
    "    return average_metrics, aggregated\n",
    "\n",
    "def plot_comparative_average_metrics(mbart_metrics, indictrans_metrics, output_dir):\n",
    "    metrics = list(mbart_metrics.keys())\n",
    "    mbart_scores = list(mbart_metrics.values())\n",
    "    indictrans_scores = list(indictrans_metrics.values())\n",
    "\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(metrics))\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.barh(index, mbart_scores, bar_width, label='mBART', color='skyblue')\n",
    "    plt.barh(index + bar_width, indictrans_scores, bar_width, label='IndicTrans', color='salmon')\n",
    "\n",
    "    plt.xlabel('Score (%)')\n",
    "    plt.yticks(index + bar_width / 2, metrics)\n",
    "    plt.legend()\n",
    "    plt.title('Comparative Average Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'comparative_average_metrics.png'))\n",
    "    plt.close()\n",
    "    print(\"Saved comparative average metrics bar plot.\")\n",
    "\n",
    "def plot_comparative_metric_distributions(mbart_aggregated, indictrans_aggregated, output_dir):\n",
    "    for metric, mbart_values in mbart_aggregated.items():\n",
    "        indictrans_values = indictrans_aggregated[metric]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(mbart_values, bins=20, alpha=0.5, label='mBART', color='skyblue', edgecolor='black')\n",
    "        plt.hist(indictrans_values, bins=20, alpha=0.5, label='IndicTrans', color='salmon', edgecolor='black')\n",
    "        \n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{metric.capitalize()} Distribution Comparison')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'comparative_{metric}_distribution.png'))\n",
    "        plt.close()\n",
    "        print(f\"Saved comparative histogram for {metric}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mbart_dir = \"/shared/3/projects/national-culture/cache/independent/cache/mbart/checkpoint-3750\"\n",
    "indictrans_dir = \"/shared/3/projects/national-culture/cache/independent/cache/indictrans/checkpoint-1875\"\n",
    "\n",
    "mbart_predictions_path = os.path.join(mbart_dir, 'mbart_predictions.txt')\n",
    "mbart_references_path = os.path.join(mbart_dir, 'mbart_references.txt')\n",
    "indictrans_predictions_path = os.path.join(indictrans_dir, 'indictrans_predictions.txt')\n",
    "indictrans_references_path = os.path.join(indictrans_dir, 'indictrans_references.txt')\n",
    "\n",
    "print(\"Loading predictions and references for mBART...\")\n",
    "mbart_predictions, mbart_references = load_texts(mbart_predictions_path, mbart_references_path)\n",
    "print(f\"Loaded {len(mbart_predictions)} predictions for mBART.\")\n",
    "\n",
    "print(\"Loading predictions and references for IndicTrans...\")\n",
    "indictrans_predictions, indictrans_references = load_texts(indictrans_predictions_path, indictrans_references_path)\n",
    "print(f\"Loaded {len(indictrans_predictions)} predictions for IndicTrans.\")\n",
    "\n",
    "print(\"Calculating metrics for mBART...\")\n",
    "mbart_average_metrics, mbart_aggregated_metrics = aggregate_metrics(mbart_predictions, mbart_references)\n",
    "\n",
    "print(\"Calculating metrics for IndicTrans...\")\n",
    "indictrans_average_metrics, indictrans_aggregated_metrics = aggregate_metrics(indictrans_predictions, indictrans_references)\n",
    "\n",
    "print(\"\\nAverage Metrics for mBART:\")\n",
    "for metric, avg_score in mbart_average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {avg_score:.2f}%\")\n",
    "\n",
    "print(\"\\nAverage Metrics for IndicTrans:\")\n",
    "for metric, avg_score in indictrans_average_metrics.items():\n",
    "    print(f\"{metric.capitalize()}: {avg_score:.2f}%\")\n",
    "\n",
    "with open(os.path.join(mbart_dir, 'mbart_average_metrics.txt'), 'w') as f:\n",
    "    for metric, avg_score in mbart_average_metrics.items():\n",
    "        f.write(f\"{metric.capitalize()}: {avg_score:.2f}%\\n\")\n",
    "\n",
    "with open(os.path.join(indictrans_dir, 'indictrans_average_metrics.txt'), 'w') as f:\n",
    "    for metric, avg_score in indictrans_average_metrics.items():\n",
    "        f.write(f\"{metric.capitalize()}: {avg_score:.2f}%\\n\")\n",
    "\n",
    "print(\"Saved average metrics to files for both models.\")\n",
    "\n",
    "output_dir = \"./\"  # Save plots in the current directory\n",
    "plot_comparative_average_metrics(mbart_average_metrics, indictrans_average_metrics, output_dir)\n",
    "\n",
    "plot_comparative_metric_distributions(mbart_aggregated_metrics, indictrans_aggregated_metrics, output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
